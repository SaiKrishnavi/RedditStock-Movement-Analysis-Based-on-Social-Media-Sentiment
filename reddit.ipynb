{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e64c753-2589-4691-8937-63867cefafee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\saikr\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saikr\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651d2551-a5ab-421d-b6c0-8483faafdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8520ab4f-dd25-4cb6-be96-ec97ed894c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"eu_W5FVaWlrBz8lydJMjcA\",          \n",
    "    client_secret=\"AFirlIb_Lic-bo_VHA1Zpue32e54Yw\",  \n",
    "    user_agent=\"Scraper\"         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26323ded-db7d-4b08-9e69-7114ba639fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = \"stocks\"  # Choose the subreddit\n",
    "query = \"stock market\"     # Search query (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1366654d-55c6-45d1-bd34-339b2d0a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "for post in subreddit.search(query, limit=100):  # Change limit as needed\n",
    "    posts.append({\n",
    "        \"Title\": post.title,\n",
    "        \"Body\": post.selftext,\n",
    "        \"Score\": post.score,\n",
    "        \"Comments\": post.num_comments,\n",
    "        \"Timestamp\": post.created_utc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351ceea6-630e-40d3-9611-02a1545c6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e83e31-4a45-47d1-9897-7d729f6fbcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit data saved!\n"
     ]
    }
   ],
   "source": [
    "df_reddit.to_csv(\"reddit_data.csv\", index=False)\n",
    "print(\"Reddit data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8494ea2-c4e2-421b-84d9-f0fa427bac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved!\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_reddit = pd.read_csv(\"reddit_data.csv\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_reddit.drop_duplicates(subset=[\"Title\", \"Body\"], inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df_reddit.fillna(\"\", inplace=True)\n",
    "\n",
    "# Combine Title and Body for analysis\n",
    "df_reddit[\"Content\"] = df_reddit[\"Title\"] + \" \" + df_reddit[\"Body\"]\n",
    "\n",
    "# Save the cleaned data\n",
    "df_reddit.to_csv(\"cleaned_reddit_data.csv\", index=False)\n",
    "print(\"Data cleaned and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c955e9d1-a5fa-413d-889a-fad02d40d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37929869-cd70-46f9-97a2-f1c2cbe20d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and saved!\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "df_reddit[\"Sentiment\"] = df_reddit[\"Content\"].apply(analyze_sentiment)\n",
    "\n",
    "# Classify sentiment as Positive, Neutral, or Negative\n",
    "df_reddit[\"Sentiment_Label\"] = df_reddit[\"Sentiment\"].apply(\n",
    "    lambda x: \"Positive\" if x > 0 else (\"Negative\" if x < 0 else \"Neutral\")\n",
    ")\n",
    "\n",
    "# Save the sentiment analysis results\n",
    "df_reddit.to_csv(\"sentiment_reddit_data.csv\", index=False)\n",
    "print(\"Sentiment analysis completed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b587bc77-9843-4524-bb81-c2d90d7f4add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00        14\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for modeling\n",
    "df_reddit[\"Stock_Movement\"] = (df_reddit[\"Sentiment\"] > 0).astype(int)  # Simplified assumption\n",
    "X = df_reddit[[\"Sentiment\", \"Score\", \"Comments\"]]\n",
    "y = df_reddit[\"Stock_Movement\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac1f95-1165-4c82-808d-eac45f2c44be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
